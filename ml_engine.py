# ml_engine.py - –†–ï–í–û–õ–Æ–¶–ò–û–ù–ù–ê–Ø –í–ï–†–°–ò–Ø
"""
–ú–æ–¥—É–ª—å –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è Creative Performance Predictor.
–ü–æ–ª–Ω–æ—Å—Ç—å—é –ø–µ—Ä–µ–ø–∏—Å–∞–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è —Å —Ä–µ–∞–ª—å–Ω—ã–º–∏ ML –º–æ–¥–µ–ª—è–º–∏ –∏ feature engineering.
"""

import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.linear_model import Ridge, ElasticNet
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.model_selection import train_test_split, cross_val_score, TimeSeriesSplit
from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error
from sklearn.feature_selection import SelectKBest, f_regression
import joblib
from typing import Dict, List, Tuple, Optional, Any
import warnings
import json
from datetime import datetime, timedelta
warnings.filterwarnings('ignore')

# –ë–µ–∑–æ–ø–∞—Å–Ω—ã–π –∏–º–ø–æ—Ä—Ç XGBoost (–û–ë–õ–ï–ì–ß–ï–ù–ù–ê–Ø –í–ï–†–°–ò–Ø)
XGBOOST_AVAILABLE = False
try:
    import xgboost as xgb
    XGBOOST_AVAILABLE = True
    print("‚úÖ XGBoost –¥–æ—Å—Ç—É–ø–µ–Ω –¥–ª—è –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–≥–æ ML")
except ImportError:
    print("‚ÑπÔ∏è XGBoost –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω. –ò—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è Random Forest –∏ Gradient Boosting.")

from config import (
    ML_MODELS, PERFORMANCE_METRICS, SYNTHETIC_DATA, 
    CREATIVE_CATEGORIES, REGIONS, FEATURE_IMPORTANCE_THRESHOLD
)

class AdvancedMLEngine:
    """
    –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –¥–≤–∏–∂–æ–∫ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –∫—Ä–µ–∞—Ç–∏–≤–æ–≤.
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏, feature engineering –∏ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—É—é –≤–∞–ª–∏–¥–∞—Ü–∏—é.
    """
    
    def __init__(self):
        # –°–ª–æ–≤–∞—Ä–∏ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π –ø–æ —Ü–µ–ª–µ–≤—ã–º –º–µ—Ç—Ä–∏–∫–∞–º
        self.models = {
            'ctr': {},
            'conversion_rate': {},
            'engagement': {}
        }
        
        # –°–∫–∞–ª–µ—Ä—ã –¥–ª—è –∫–∞–∂–¥–æ–π –º–µ—Ç—Ä–∏–∫–∏
        self.scalers = {
            'ctr': StandardScaler(),
            'conversion_rate': RobustScaler(),  # –ë–æ–ª–µ–µ —É—Å—Ç–æ–π—á–∏–≤ –∫ –≤—ã–±—Ä–æ—Å–∞–º
            'engagement': StandardScaler()
        }
        
        # Feature engineering
        self.feature_selectors = {}
        self.feature_names = []
        self.feature_importance = {}
        self.model_performance = {}
        
        # –°—Ç–∞—Ç—É—Å –æ–±—É—á–µ–Ω–∏—è
        self.is_trained = False
        self.training_data = None
        self.training_timestamp = None
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–µ–π
        self._initialize_models()
        
        # –í–µ—Å–∞ –¥–ª—è –∞–Ω—Å–∞–º–±–ª—è –º–æ–¥–µ–ª–µ–π
        self.ensemble_weights = {
            'ctr': {'random_forest': 0.4, 'gradient_boosting': 0.3, 'elastic_net': 0.3},
            'conversion_rate': {'random_forest': 0.3, 'gradient_boosting': 0.4, 'elastic_net': 0.3},
            'engagement': {'random_forest': 0.5, 'gradient_boosting': 0.3, 'elastic_net': 0.2}
        }
        
    def _initialize_models(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã—Ö ML –º–æ–¥–µ–ª–µ–π."""
        
        # Random Forest (–±–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å)
        rf_params = {
            'n_estimators': 200,
            'max_depth': 12,
            'min_samples_split': 5,
            'min_samples_leaf': 2,
            'max_features': 'sqrt',
            'random_state': 42,
            'n_jobs': -1
        }
        
        # Gradient Boosting (–ø—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è –º–æ–¥–µ–ª—å)
        gb_params = {
            'n_estimators': 150,
            'max_depth': 6,
            'learning_rate': 0.1,
            'min_samples_split': 5,
            'min_samples_leaf': 2,
            'subsample': 0.8,
            'random_state': 42
        }
        
        # Elastic Net (–ª–∏–Ω–µ–π–Ω–∞—è –º–æ–¥–µ–ª—å —Å —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–µ–π)
        en_params = {
            'alpha': 1.0,
            'l1_ratio': 0.5,
            'random_state': 42,
            'max_iter': 2000
        }
        
        # XGBoost (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω)
        if XGBOOST_AVAILABLE:
            xgb_params = {
                'n_estimators': 100,
                'max_depth': 6,
                'learning_rate': 0.1,
                'subsample': 0.8,
                'colsample_bytree': 0.8,
                'random_state': 42,
                'objective': 'reg:squarederror'
            }
        
        # –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª–∏ –¥–ª—è –∫–∞–∂–¥–æ–π —Ü–µ–ª–µ–≤–æ–π –º–µ—Ç—Ä–∏–∫–∏
        for target in ['ctr', 'conversion_rate', 'engagement']:
            self.models[target] = {
                'random_forest': RandomForestRegressor(**rf_params),
                'gradient_boosting': GradientBoostingRegressor(**gb_params),
                'elastic_net': ElasticNet(**en_params)
            }
            
            if XGBOOST_AVAILABLE:
                self.models[target]['xgboost'] = xgb.XGBRegressor(**xgb_params)
    
    def generate_realistic_data(self, n_samples: int = 1000) -> pd.DataFrame:
        """
        –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π –º–∞—Ä–∫–µ—Ç–∏–Ω–≥–æ–≤–æ–π —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏.
        """
        np.random.seed(42)
        
        # === –û–°–ù–û–í–ù–´–ï –í–ò–ó–£–ê–õ–¨–ù–´–ï –ü–†–ò–ó–ù–ê–ö–ò ===
        data = {}
        
        # –¶–≤–µ—Ç–æ–≤—ã–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ (–æ—Å–Ω–æ–≤–∞–Ω—ã –Ω–∞ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è—Ö —Ü–≤–µ—Ç–æ–≤–æ–π –ø—Å–∏—Ö–æ–ª–æ–≥–∏–∏)
        data['brightness'] = np.random.beta(2.5, 2.5, n_samples)  # –£–º–µ—Ä–µ–Ω–Ω–∞—è —è—Ä–∫–æ—Å—Ç—å —Ä–∞–±–æ—Ç–∞–µ—Ç –ª—É—á—à–µ
        data['saturation'] = np.random.beta(3, 2, n_samples)      # –í—ã—Å–æ–∫–∞—è –Ω–∞—Å—ã—â–µ–Ω–Ω–æ—Å—Ç—å –ø—Ä–∏–≤–ª–µ–∫–∞–µ—Ç
        data['contrast_score'] = np.random.beta(3, 2, n_samples)  # –í—ã—Å–æ–∫–∏–π –∫–æ–Ω—Ç—Ä–∞—Å—Ç –≤–∞–∂–µ–Ω
        data['color_temperature'] = np.random.beta(2, 2, n_samples)  # –ë–∞–ª–∞–Ω—Å —Ç–µ–ø–ª—ã—Ö –∏ —Ö–æ–ª–æ–¥–Ω—ã—Ö
        data['harmony_score'] = np.random.beta(2.5, 2, n_samples)    # –ì–∞—Ä–º–æ–Ω–∏—è –≤–∞–∂–Ω–∞
        data['color_vibrancy'] = np.random.beta(2.5, 2, n_samples)   # –ñ–∏–≤–æ—Å—Ç—å —Ü–≤–µ—Ç–æ–≤
        data['emotional_impact'] = np.random.beta(2, 2, n_samples)   # –≠–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–µ –≤–æ–∑–¥–µ–π—Å—Ç–≤–∏–µ
        
        # –ö–æ–º–ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏
        data['rule_of_thirds_score'] = np.random.beta(2, 3, n_samples)  # –ù–µ –≤—Å–µ —Å–ª–µ–¥—É—é—Ç –ø—Ä–∞–≤–∏–ª—É
        data['visual_balance_score'] = np.random.beta(3, 2, n_samples)  # –ë–∞–ª–∞–Ω—Å –≤–∞–∂–µ–Ω
        data['composition_complexity'] = np.random.beta(2, 3, n_samples)  # –ü—Ä–æ—Å—Ç–æ—Ç–∞ –ª—É—á—à–µ
        data['center_focus_score'] = np.random.beta(2.5, 2.5, n_samples)  # –°—Ä–µ–¥–Ω–∏–π —Ñ–æ–∫—É—Å
        data['symmetry_score'] = np.random.beta(2, 3, n_samples)          # –ù–µ–º–Ω–æ–≥–æ –∞—Å–∏–º–º–µ—Ç—Ä–∏–∏
        data['negative_space'] = np.random.beta(2.5, 2, n_samples)        # –í–∞–∂–Ω–æ –¥–ª—è –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è
        data['visual_flow'] = np.random.beta(2.5, 2.5, n_samples)         # –î–≤–∏–∂–µ–Ω–∏–µ –≤–∑–≥–ª—è–¥–∞
        
        # –¢–µ–∫—Å—Ç–æ–≤—ã–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏
        # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–µ–∫—Å—Ç–∞ (–¥–∏—Å–∫—Ä–µ—Ç–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ)
        text_amounts = np.random.choice([0, 1, 2, 3, 4, 5, 6], n_samples, 
                                      p=[0.1, 0.2, 0.25, 0.2, 0.15, 0.08, 0.02])
        data['text_amount'] = text_amounts / 6.0  # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
        
        # –ö–∞—á–µ—Å—Ç–≤–æ —Ç–µ–∫—Å—Ç–∞ –∑–∞–≤–∏—Å–∏—Ç –æ—Ç –µ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞
        data['readability_score'] = np.where(
            text_amounts > 0,
            np.random.beta(3, 2, n_samples),  # –ï—Å–ª–∏ –µ—Å—Ç—å —Ç–µ–∫—Å—Ç, –æ–±—ã—á–Ω–æ —á–∏—Ç–∞–µ–º—ã–π
            1.0  # –ï—Å–ª–∏ —Ç–µ–∫—Å—Ç–∞ –Ω–µ—Ç, —á–∏—Ç–∞–µ–º–æ—Å—Ç—å –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è
        )
        
        data['text_hierarchy'] = np.where(
            text_amounts > 1,
            np.random.beta(2.5, 2.5, n_samples),  # –ò–µ—Ä–∞—Ä—Ö–∏—è –≤–∞–∂–Ω–∞ –ø—Ä–∏ –º–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–µ
            1.0  # –ï—Å–ª–∏ –º–∞–ª–æ —Ç–µ–∫—Å—Ç–∞, –∏–µ—Ä–∞—Ä—Ö–∏—è –Ω–µ –∫—Ä–∏—Ç–∏—á–Ω–∞
        )
        
        data['text_contrast'] = np.where(
            text_amounts > 0,
            np.random.beta(3, 2, n_samples),  # –ö–æ–Ω—Ç—Ä–∞—Å—Ç –≤–∞–∂–µ–Ω –¥–ª—è –ª—é–±–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
            1.0
        )
        
        # –ü—Ä–∏–∑—ã–≤ –∫ –¥–µ–π—Å—Ç–≤–∏—é (binary —Å –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å—é)
        cta_probability = np.where(text_amounts > 0, 0.7, 0.1)  # –ë–æ–ª—å—à–µ —à–∞–Ω—Å–æ–≤ –ø—Ä–∏ –Ω–∞–ª–∏—á–∏–∏ —Ç–µ–∫—Å—Ç–∞
        data['has_cta'] = np.random.binomial(1, cta_probability)
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        data['aspect_ratio'] = np.clip(np.random.lognormal(0, 0.3, n_samples), 0.3, 3.0) / 3.0
        data['overall_complexity'] = np.random.beta(2, 3, n_samples)  # –ü—Ä–æ—Å—Ç–æ—Ç–∞ –ª—É—á—à–µ
        data['visual_appeal'] = np.random.beta(2.5, 2, n_samples)     # –û–±—â–∞—è –ø—Ä–∏–≤–ª–µ–∫–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å
        
        # === –ö–ê–¢–ï–ì–û–†–ò–ê–õ–¨–ù–´–ï –ü–ï–†–ï–ú–ï–ù–ù–´–ï ===
        categories = ['–ê–≤—Ç–æ–º–æ–±–∏–ª–∏', 'E-commerce', '–§–∏–Ω–∞–Ω—Å—ã', '–¢–µ—Ö–Ω–æ–ª–æ–≥–∏–∏', '–ó–¥–æ—Ä–æ–≤—å–µ', '–û–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ']
        regions = ['–†–æ—Å—Å–∏—è', '–°–®–ê', '–ï–≤—Ä–æ–ø–∞', '–ê–∑–∏—è']
        
        data['category'] = np.random.choice(categories, n_samples)
        data['region'] = np.random.choice(regions, n_samples)
        
        # === –í–†–ï–ú–ï–ù–ù–´–ï –§–ê–ö–¢–û–†–´ ===
        # –≠–º—É–ª—è—Ü–∏—è —Å–µ–∑–æ–Ω–Ω–æ—Å—Ç–∏ –∏ —Ç—Ä–µ–Ω–¥–æ–≤
        days = np.random.randint(0, 365, n_samples)
        data['seasonality'] = 0.5 + 0.3 * np.sin(2 * np.pi * days / 365)  # –ì–æ–¥–æ–≤–∞—è —Å–µ–∑–æ–Ω–Ω–æ—Å—Ç—å
        data['weekly_trend'] = 0.5 + 0.2 * np.sin(2 * np.pi * days / 7)   # –ù–µ–¥–µ–ª—å–Ω—ã–π —Ç—Ä–µ–Ω–¥
        
        df = pd.DataFrame(data)
        
        # === –ì–ï–ù–ï–†–ê–¶–ò–Ø –¶–ï–õ–ï–í–´–• –ú–ï–¢–†–ò–ö ===
        df = self._generate_realistic_targets(df)
        
        return df
    
    def _generate_realistic_targets(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã—Ö —Ü–µ–ª–µ–≤—ã—Ö –º–µ—Ç—Ä–∏–∫ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π –∏ –∑–¥—Ä–∞–≤–æ–≥–æ —Å–º—ã—Å–ª–∞.
        """
        n_samples = len(df)
        
        # === CTR –ú–û–î–ï–õ–¨ (Click-Through Rate) ===
        # –û—Å–Ω–æ–≤–∞–Ω–æ –Ω–∞ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è—Ö —Ñ–∞–∫—Ç–æ—Ä–æ–≤, –≤–ª–∏—è—é—â–∏—Ö –Ω–∞ CTR
        
        ctr_base = (
            # –¶–≤–µ—Ç–æ–≤—ã–µ —Ñ–∞–∫—Ç–æ—Ä—ã (30% –≤–ª–∏—è–Ω–∏—è)
            0.15 * df['contrast_score'] +           # –ö–æ–Ω—Ç—Ä–∞—Å—Ç –∫—Ä–∏—Ç–∏—á–µ–Ω –¥–ª—è –ø—Ä–∏–≤–ª–µ—á–µ–Ω–∏—è –≤–Ω–∏–º–∞–Ω–∏—è
            0.08 * df['color_vibrancy'] +           # –Ø—Ä–∫–∏–µ —Ü–≤–µ—Ç–∞ –ø—Ä–∏–≤–ª–µ–∫–∞—é—Ç
            0.07 * df['emotional_impact'] +         # –≠–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å –≤–∞–∂–Ω–∞
            
            # –ö–æ–º–ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã–µ —Ñ–∞–∫—Ç–æ—Ä—ã (40% –≤–ª–∏—è–Ω–∏—è)
            0.12 * df['visual_appeal'] +            # –û–±—â–∞—è –ø—Ä–∏–≤–ª–µ–∫–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å
            0.10 * (1 - df['composition_complexity']) +  # –ü—Ä–æ—Å—Ç–æ—Ç–∞ –ª—É—á—à–µ –¥–ª—è CTR
            0.08 * df['rule_of_thirds_score'] +     # –ü—Ä–∞–≤–∏–ª—å–Ω–∞—è –∫–æ–º–ø–æ–∑–∏—Ü–∏—è
            0.10 * df['negative_space'] +           # –ü—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ –¥–ª—è –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è
            
            # –¢–µ–∫—Å—Ç–æ–≤—ã–µ —Ñ–∞–∫—Ç–æ—Ä—ã (20% –≤–ª–∏—è–Ω–∏—è)
            0.12 * df['has_cta'] +                  # CTA –∫—Ä–∏—Ç–∏—á–µ–Ω –¥–ª—è –∫–ª–∏–∫–æ–≤
            0.08 * df['text_contrast'] +            # –ß–∏—Ç–∞–µ–º–æ—Å—Ç—å –≤–∞–∂–Ω–∞
            
            # –ö–æ–Ω—Ç–µ–∫—Å—Ç—É–∞–ª—å–Ω—ã–µ —Ñ–∞–∫—Ç–æ—Ä—ã (10% –≤–ª–∏—è–Ω–∏—è)
            0.05 * df['seasonality'] +              # –°–µ–∑–æ–Ω–Ω—ã–µ –∫–æ–ª–µ–±–∞–Ω–∏—è
            0.05 * df['weekly_trend']               # –ù–µ–¥–µ–ª—å–Ω—ã–µ —Ç—Ä–µ–Ω–¥—ã
        )
        
        # –ö–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –º–æ–¥–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã –¥–ª—è CTR
        category_ctr_multipliers = {
            'E-commerce': 1.3,      # E-commerce –æ–±—ã—á–Ω–æ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –≤—ã—Å–æ–∫–∏–π CTR
            '–ê–≤—Ç–æ–º–æ–±–∏–ª–∏': 1.1,      # –ê–≤—Ç–æ–º–æ–±–∏–ª–∏ –ø—Ä–∏–≤–ª–µ–∫–∞—é—Ç –≤–Ω–∏–º–∞–Ω–∏–µ
            '–¢–µ—Ö–Ω–æ–ª–æ–≥–∏–∏': 1.0,      # –ë–∞–∑–æ–≤—ã–π —É—Ä–æ–≤–µ–Ω—å
            '–§–∏–Ω–∞–Ω—Å—ã': 0.8,         # –ö–æ–Ω—Å–µ—Ä–≤–∞—Ç–∏–≤–Ω–∞—è —Å—Ñ–µ—Ä–∞
            '–ó–¥–æ—Ä–æ–≤—å–µ': 0.9,        # –¢—Ä–µ–±—É–µ—Ç –¥–æ–≤–µ—Ä–∏—è
            '–û–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ': 0.85     # –°–ø–µ—Ü–∏—Ñ–∏—á–µ—Å–∫–∞—è –∞—É–¥–∏—Ç–æ—Ä–∏—è
        }
        
        region_ctr_multipliers = {
            '–°–®–ê': 1.2,             # –†–∞–∑–≤–∏—Ç—ã–π —Ä—ã–Ω–æ–∫ –æ–Ω–ª–∞–π–Ω —Ä–µ–∫–ª–∞–º—ã
            '–ï–≤—Ä–æ–ø–∞': 1.0,          # –ë–∞–∑–æ–≤—ã–π —É—Ä–æ–≤–µ–Ω—å
            '–†–æ—Å—Å–∏—è': 0.9,          # –†–∞–∑–≤–∏–≤–∞—é—â–∏–π—Å—è —Ä—ã–Ω–æ–∫
            '–ê–∑–∏—è': 1.1             # –ê–∫—Ç–∏–≤–Ω—ã–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–∞
        }
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º –º–æ–¥–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã
        for category, multiplier in category_ctr_multipliers.items():
            mask = df['category'] == category
            ctr_base[mask] *= multiplier
            
        for region, multiplier in region_ctr_multipliers.items():
            mask = df['region'] == region
            ctr_base[mask] *= multiplier
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–π —à—É–º –∏ –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º
        ctr_noise = np.random.normal(0, 0.05, n_samples)
        df['ctr'] = np.clip(ctr_base + ctr_noise, 0.001, 0.15)
        
        # === CONVERSION RATE –ú–û–î–ï–õ–¨ ===
        # –û—Å–Ω–æ–≤–∞–Ω–æ –Ω–∞ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è—Ö UX –∏ –∫–æ–Ω–≤–µ—Ä—Å–∏–æ–Ω–Ω–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
        
        conv_base = (
            # –î–æ–≤–µ—Ä–∏–µ –∏ —è—Å–Ω–æ—Å—Ç—å (50% –≤–ª–∏—è–Ω–∏—è)
            0.20 * df['readability_score'] +        # –ß–∏—Ç–∞–µ–º–æ—Å—Ç—å –∫—Ä–∏—Ç–∏—á–Ω–∞
            0.15 * df['text_hierarchy'] +           # –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
            0.15 * df['has_cta'] +                  # –ß–µ—Ç–∫–∏–π CTA
            
            # –í–∏–∑—É–∞–ª—å–Ω–∞—è –ø—Ä–∏–≤–ª–µ–∫–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å (30% –≤–ª–∏—è–Ω–∏—è)
            0.12 * df['visual_appeal'] +            # –û–±—â–∞—è –ø—Ä–∏–≤–ª–µ–∫–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å
            0.10 * df['harmony_score'] +            # –ì–∞—Ä–º–æ–Ω–∏—è –≤—ã–∑—ã–≤–∞–µ—Ç –¥–æ–≤–µ—Ä–∏–µ
            0.08 * (1 - df['overall_complexity']) + # –ü—Ä–æ—Å—Ç–æ—Ç–∞ –ø–æ–º–æ–≥–∞–µ—Ç —Ä–µ—à–µ–Ω–∏—é
            
            # –≠–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–µ –≤–æ–∑–¥–µ–π—Å—Ç–≤–∏–µ (20% –≤–ª–∏—è–Ω–∏—è)
            0.12 * df['emotional_impact'] +         # –≠–º–æ—Ü–∏–∏ –≤–ª–∏—è—é—Ç –Ω–∞ —Ä–µ—à–µ–Ω–∏—è
            0.08 * df['visual_flow']                # –õ–æ–≥–∏—á–Ω–æ—Å—Ç—å –ø–æ–¥–∞—á–∏
        )
        
        # –ö–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –º–æ–¥–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã –¥–ª—è –∫–æ–Ω–≤–µ—Ä—Å–∏–π
        category_conv_multipliers = {
            'E-commerce': 1.4,      # E-commerce –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω –¥–ª—è –∫–æ–Ω–≤–µ—Ä—Å–∏–π
            '–§–∏–Ω–∞–Ω—Å—ã': 1.2,         # –í—ã—Å–æ–∫–∞—è —Ü–µ–Ω–Ω–æ—Å—Ç—å –∫–æ–Ω–≤–µ—Ä—Å–∏–π
            '–ó–¥–æ—Ä–æ–≤—å–µ': 1.1,        # –ú–æ—Ç–∏–≤–∞—Ü–∏—è –∫ –¥–µ–π—Å—Ç–≤–∏—é
            '–¢–µ—Ö–Ω–æ–ª–æ–≥–∏–∏': 1.0,      # –ë–∞–∑–æ–≤—ã–π —É—Ä–æ–≤–µ–Ω—å
            '–ê–≤—Ç–æ–º–æ–±–∏–ª–∏': 0.8,      # –î–ª–∏–Ω–Ω—ã–π —Ü–∏–∫–ª –ø—Ä–∏–Ω—è—Ç–∏—è —Ä–µ—à–µ–Ω–∏–π
            '–û–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ': 0.9      # –û–±–¥—É–º–∞–Ω–Ω—ã–µ —Ä–µ—à–µ–Ω–∏—è
        }
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º –º–æ–¥–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã
        for category, multiplier in category_conv_multipliers.items():
            mask = df['category'] == category
            conv_base[mask] *= multiplier
            
        for region, multiplier in region_ctr_multipliers.items():  # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–µ –∂–µ —Ä–µ–≥–∏–æ–Ω–∞–ª—å–Ω—ã–µ
            mask = df['region'] == region
            conv_base[mask] *= multiplier * 0.8  # –ù–æ —Å –º–µ–Ω—å—à–∏–º –≤–ª–∏—è–Ω–∏–µ–º
        
        conv_noise = np.random.normal(0, 0.03, n_samples)
        df['conversion_rate'] = np.clip(conv_base + conv_noise, 0.001, 0.35)
        
        # === ENGAGEMENT –ú–û–î–ï–õ–¨ ===
        # –û—Å–Ω–æ–≤–∞–Ω–æ –Ω–∞ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è—Ö —Å–æ—Ü–∏–∞–ª—å–Ω—ã—Ö –º–µ–¥–∏–∞ –∏ –≤–æ–≤–ª–µ—á–µ–Ω–Ω–æ—Å—Ç–∏
        
        eng_base = (
            # –≠–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–µ —Ñ–∞–∫—Ç–æ—Ä—ã (40% –≤–ª–∏—è–Ω–∏—è)
            0.20 * df['emotional_impact'] +         # –≠–º–æ—Ü–∏–∏ = –≤–æ–≤–ª–µ—á–µ–Ω–Ω–æ—Å—Ç—å
            0.10 * df['color_vibrancy'] +           # –Ø—Ä–∫–æ—Å—Ç—å –ø—Ä–∏–≤–ª–µ–∫–∞–µ—Ç
            0.10 * df['visual_appeal'] +            # –ö—Ä–∞—Å–æ—Ç–∞ –º–æ—Ç–∏–≤–∏—Ä—É–µ—Ç
            
            # –ò–Ω—Ç–µ—Ä–µ—Å–Ω–æ—Å—Ç—å –∫–æ–Ω—Ç–µ–Ω—Ç–∞ (35% –≤–ª–∏—è–Ω–∏—è)
            0.15 * (1 - df['overall_complexity']) + # –ü—Ä–æ—Å—Ç–æ—Ç–∞ –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è
            0.10 * df['visual_flow'] +              # –õ–æ–≥–∏—á–Ω–æ—Å—Ç—å –ø–æ–¥–∞—á–∏
            0.10 * df['rule_of_thirds_score'] +     # –ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π –≤–∏–¥
            
            # –ö–∞—á–µ—Å—Ç–≤–æ –∏—Å–ø–æ–ª–Ω–µ–Ω–∏—è (25% –≤–ª–∏—è–Ω–∏—è)
            0.12 * df['harmony_score'] +            # –ì–∞—Ä–º–æ–Ω–∏—è = –∫–∞—á–µ—Å—Ç–≤–æ
            0.08 * df['text_contrast'] +            # –ß–∏—Ç–∞–µ–º–æ—Å—Ç—å
            0.05 * df['seasonality']                # –°–µ–∑–æ–Ω–Ω—ã–µ —Ñ–∞–∫—Ç–æ—Ä—ã
        )
        
        # –ö–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –º–æ–¥–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã –¥–ª—è engagement
        category_eng_multipliers = {
            '–¢–µ—Ö–Ω–æ–ª–æ–≥–∏–∏': 1.3,      # Tech-–∞—É–¥–∏—Ç–æ—Ä–∏—è –∞–∫—Ç–∏–≤–Ω–∞
            '–ê–≤—Ç–æ–º–æ–±–∏–ª–∏': 1.2,      # –≠–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è
            'E-commerce': 1.0,      # –ë–∞–∑–æ–≤—ã–π —É—Ä–æ–≤–µ–Ω—å
            '–ó–¥–æ—Ä–æ–≤—å–µ': 0.9,        # –ë–æ–ª–µ–µ —Å–µ—Ä—å–µ–∑–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç
            '–§–∏–Ω–∞–Ω—Å—ã': 0.8,         # –ö–æ–Ω—Å–µ—Ä–≤–∞—Ç–∏–≤–Ω–∞—è –∞—É–¥–∏—Ç–æ—Ä–∏—è
            '–û–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ': 0.85     # –°–ø–µ—Ü–∏—Ñ–∏—á–µ—Å–∫–∞—è –∞—É–¥–∏—Ç–æ—Ä–∏—è
        }
        
        for category, multiplier in category_eng_multipliers.items():
            mask = df['category'] == category
            eng_base[mask] *= multiplier
        
        eng_noise = np.random.normal(0, 0.04, n_samples)
        df['engagement'] = np.clip(eng_base + eng_noise, 0.01, 0.8)
        
        # === –ö–û–†–†–ï–õ–Ø–¶–ò–ò –ú–ï–ñ–î–£ –ú–ï–¢–†–ò–ö–ê–ú–ò ===
        # –í —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏ –º–µ—Ç—Ä–∏–∫–∏ –∫–æ—Ä—Ä–µ–ª–∏—Ä—É—é—Ç, –¥–æ–±–∞–≤–ª—è–µ–º —ç—Ç–æ
        
        # CTR –≤–ª–∏—è–µ—Ç –Ω–∞ –∫–æ–Ω–≤–µ—Ä—Å–∏–∏ (–Ω–æ –Ω–µ –ª–∏–Ω–µ–π–Ω–æ)
        ctr_effect = np.sqrt(df['ctr']) * 0.3
        df['conversion_rate'] = np.clip(df['conversion_rate'] + ctr_effect, 0.001, 0.35)
        
        # Engagement –≤–ª–∏—è–µ—Ç –Ω–∞ CTR
        eng_effect = np.sqrt(df['engagement']) * 0.1
        df['ctr'] = np.clip(df['ctr'] + eng_effect, 0.001, 0.15)
        
        return df
    
    def prepare_features(self, df: pd.DataFrame) -> Tuple[Dict[str, np.ndarray], List[str]]:
        """
        –ü—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å feature engineering.
        """
        # –ò—Å–∫–ª—é—á–∞–µ–º —Ü–µ–ª–µ–≤—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –∏ –≤—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ
        exclude_columns = ['ctr', 'conversion_rate', 'engagement', 'seasonality', 'weekly_trend']
        
        # One-hot encoding –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö
        df_encoded = pd.get_dummies(df, columns=['category', 'region'], prefix=['cat', 'reg'])
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (feature engineering)
        df_encoded = self._create_engineered_features(df_encoded)
        
        # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        feature_columns = [col for col in df_encoded.columns if col not in exclude_columns]
        
        # –°–æ–∑–¥–∞–µ–º –æ—Ç–¥–µ–ª—å–Ω—ã–µ –Ω–∞–±–æ—Ä—ã –¥–ª—è –∫–∞–∂–¥–æ–π —Ü–µ–ª–µ–≤–æ–π –º–µ—Ç—Ä–∏–∫–∏
        feature_sets = {}
        
        for target in ['ctr', 'conversion_rate', 'engagement']:
            X = df_encoded[feature_columns].values
            feature_sets[target] = X
        
        self.feature_names = feature_columns
        return feature_sets, feature_columns
    
    def _create_engineered_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """–°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö."""
        
        # –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        df['color_composition_score'] = df['harmony_score'] * df['visual_appeal']
        df['text_effectiveness'] = df['readability_score'] * df['text_contrast'] * (df['has_cta'] + 0.1)
        df['visual_impact'] = df['contrast_score'] * df['color_vibrancy'] * df['visual_appeal']
        
        # –ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–µ –∏–Ω–¥–µ–∫—Å—ã
        df['simplicity_index'] = (2 - df['composition_complexity'] - df['overall_complexity']) / 2
        df['professionalism_index'] = (df['harmony_score'] + df['visual_balance_score'] + df['rule_of_thirds_score']) / 3
        
        # –ö–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è
        if 'cat_E-commerce' in df.columns:
            df['ecommerce_cta_boost'] = df['cat_E-commerce'] * df['has_cta'] * 2
        if 'cat_–§–∏–Ω–∞–Ω—Å—ã' in df.columns:
            df['finance_trust_index'] = df['cat_–§–∏–Ω–∞–Ω—Å—ã'] * df['professionalism_index']
        
        # –ü–æ—Ä–æ–≥–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        df['high_contrast'] = (df['contrast_score'] > 0.7).astype(int)
        df['has_text'] = (df['text_amount'] > 0.1).astype(int)
        df['complex_design'] = (df['overall_complexity'] > 0.6).astype(int)
        
        return df
    
    def train_models(self, df: pd.DataFrame = None, quick_mode: bool = False) -> Dict[str, Dict]:
        """
        –û–±—É—á–µ–Ω–∏–µ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã—Ö ML –º–æ–¥–µ–ª–µ–π —Å –≤–∞–ª–∏–¥–∞—Ü–∏–µ–π.
        """
        if df is None:
            sample_size = 800 if not quick_mode else 500
            df = self.generate_realistic_data(sample_size)
        
        self.training_data = df
        self.training_timestamp = datetime.now()
        
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        feature_sets, feature_names = self.prepare_features(df)
        
        results = {}
        
        # –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª–∏ –¥–ª—è –∫–∞–∂–¥–æ–π —Ü–µ–ª–µ–≤–æ–π –º–µ—Ç—Ä–∏–∫–∏ –æ—Ç–¥–µ–ª—å–Ω–æ
        for target in ['ctr', 'conversion_rate', 'engagement']:
            print(f"üéØ –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –¥–ª—è {target}...")
            
            X = feature_sets[target]
            y = df[target].values
            
            # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train/test
            X_train, X_test, y_train, y_test = train_test_split(
                X, y, test_size=0.2, random_state=42, shuffle=True
            )
            
            # –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ
            X_train_scaled = self.scalers[target].fit_transform(X_train)
            X_test_scaled = self.scalers[target].transform(X_test)
            
            # Feature selection
            if not quick_mode:
                selector = SelectKBest(score_func=f_regression, k=min(25, X_train.shape[1]))
                X_train_selected = selector.fit_transform(X_train_scaled, y_train)
                X_test_selected = selector.transform(X_test_scaled)
                self.feature_selectors[target] = selector
            else:
                X_train_selected = X_train_scaled
                X_test_selected = X_test_scaled
            
            target_results = {}
            
            # –û–±—É—á–∞–µ–º –∫–∞–∂–¥—É—é –º–æ–¥–µ–ª—å
            for model_name, model in self.models[target].items():
                try:
                    if quick_mode and model_name == 'xgboost':
                        continue  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º XGBoost –≤ –±—ã—Å—Ç—Ä–æ–º —Ä–µ–∂–∏–º–µ
                    
                    # –û–±—É—á–µ–Ω–∏–µ
                    model.fit(X_train_selected, y_train)
                    
                    # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
                    y_pred = model.predict(X_test_selected)
                    
                    # –ú–µ—Ç—Ä–∏–∫–∏
                    r2 = r2_score(y_test, y_pred)
                    mae = mean_absolute_error(y_test, y_pred)
                    rmse = np.sqrt(mean_squared_error(y_test, y_pred))
                    
                    # –ö—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è
                    if not quick_mode:
                        cv_scores = cross_val_score(model, X_train_selected, y_train, cv=5, scoring='r2')
                        cv_mean = cv_scores.mean()
                        cv_std = cv_scores.std()
                    else:
                        cv_mean, cv_std = r2, 0.0
                    
                    target_results[model_name] = {
                        'r2_score': r2,
                        'mae': mae,
                        'rmse': rmse,
                        'cv_r2_mean': cv_mean,
                        'cv_r2_std': cv_std
                    }
                    
                    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
                    if hasattr(model, 'feature_importances_'):
                        if target not in self.feature_importance:
                            self.feature_importance[target] = {}
                        
                        if not quick_mode and target in self.feature_selectors:
                            # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –≤–∞–∂–Ω–æ—Å—Ç—å –¥–ª—è –≤—Å–µ—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
                            selected_features = self.feature_selectors[target].get_support()
                            full_importance = np.zeros(len(feature_names))
                            full_importance[selected_features] = model.feature_importances_
                            importance_dict = dict(zip(feature_names, full_importance))
                        else:
                            importance_dict = dict(zip(feature_names, model.feature_importances_))
                        
                        self.feature_importance[target][model_name] = importance_dict
                    
                    print(f"  ‚úÖ {model_name}: R¬≤ = {r2:.3f}, MAE = {mae:.4f}")
                    
                except Exception as e:
                    print(f"  ‚ùå {model_name}: {str(e)}")
                    target_results[model_name] = {'error': str(e)}
            
            results[target] = target_results
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ–±—â–∏—Ö –º–µ—Ç—Ä–∏–∫
        self.model_performance = results
        self.is_trained = True
        
        print("üéâ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
        return results
    
    def predict(self, features: Dict[str, Any]) -> Dict[str, float]:
        """
        –ü—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∞–Ω—Å–∞–º–±–ª—è –º–æ–¥–µ–ª–µ–π.
        """
        if not self.is_trained:
            raise ValueError("–ú–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞. –í—ã–∑–æ–≤–∏—Ç–µ train_models() —Å–Ω–∞—á–∞–ª–∞.")
        
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        feature_vector = self._prepare_prediction_features(features)
        
        predictions = {}
        
        for target in ['ctr', 'conversion_rate', 'engagement']:
            # –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ
            feature_vector_scaled = self.scalers[target].transform([feature_vector])
            
            # Feature selection (–µ—Å–ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∞—Å—å)
            if target in self.feature_selectors:
                feature_vector_selected = self.feature_selectors[target].transform(feature_vector_scaled)
            else:
                feature_vector_selected = feature_vector_scaled
            
            # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –æ—Ç –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π
            model_predictions = []
            model_weights = []
            
            for model_name, model in self.models[target].items():
                try:
                    if hasattr(model, 'predict'):  # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –º–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞
                        pred = model.predict(feature_vector_selected)[0]
                        weight = self.ensemble_weights[target].get(model_name, 0.3)
                        
                        model_predictions.append(pred)
                        model_weights.append(weight)
                except:
                    continue
            
            if model_predictions:
                # –í–∑–≤–µ—à–µ–Ω–Ω–æ–µ —Å—Ä–µ–¥–Ω–µ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
                total_weight = sum(model_weights)
                weighted_pred = sum(p * w for p, w in zip(model_predictions, model_weights)) / total_weight
                
                # –ü—Ä–∏–º–µ–Ω—è–µ–º –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –¥–ª—è —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω–æ—Å—Ç–∏
                if target == 'ctr':
                    predictions[target] = np.clip(weighted_pred, 0.001, 0.15)
                elif target == 'conversion_rate':
                    predictions[target] = np.clip(weighted_pred, 0.001, 0.35)
                else:  # engagement
                    predictions[target] = np.clip(weighted_pred, 0.01, 0.8)
            else:
                # Fallback –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
                fallback_values = {'ctr': 0.02, 'conversion_rate': 0.05, 'engagement': 0.1}
                predictions[target] = fallback_values[target]
        
        return predictions
    
    def _prepare_prediction_features(self, features: Dict[str, Any]) -> np.ndarray:
        """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è."""
        # –°–æ–∑–¥–∞–µ–º –≤–µ–∫—Ç–æ—Ä —Å –∑–Ω–∞—á–µ–Ω–∏—è–º–∏ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        feature_vector = {}
        
        # –ó–∞–ø–æ–ª–Ω—è–µ–º –æ—Å–Ω–æ–≤–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        for feature_name in self.feature_names:
            if feature_name.startswith('cat_') or feature_name.startswith('reg_'):
                # –ö–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
                feature_vector[feature_name] = 0.0
            else:
                # –ß–∏—Å–ª–µ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
                feature_vector[feature_name] = features.get(feature_name, 0.5)
        
        # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        if 'category' in features:
            cat_feature = f"cat_{features['category']}"
            if cat_feature in feature_vector:
                feature_vector[cat_feature] = 1.0
        
        if 'region' in features:
            reg_feature = f"reg_{features['region']}"
            if reg_feature in feature_vector:
                feature_vector[reg_feature] = 1.0
        
        # –°–æ–∑–¥–∞–µ–º engineered features –µ—Å–ª–∏ –æ–Ω–∏ –µ—Å—Ç—å –≤ –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö
        self._add_engineered_features_to_vector(feature_vector, features)
        
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ –º–∞—Å—Å–∏–≤ –≤ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º –ø–æ—Ä—è–¥–∫–µ
        ordered_values = [feature_vector[name] for name in self.feature_names]
        return np.array(ordered_values)
    
    def _add_engineered_features_to_vector(self, feature_vector: Dict, original_features: Dict):
        """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ engineered –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –≤ –≤–µ–∫—Ç–æ—Ä –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è."""
        # –í–æ—Å—Å–æ–∑–¥–∞–µ–º engineered features
        if 'color_composition_score' in feature_vector:
            harmony = original_features.get('harmony_score', 0.5)
            visual_appeal = original_features.get('visual_appeal', 0.5)
            feature_vector['color_composition_score'] = harmony * visual_appeal
        
        if 'text_effectiveness' in feature_vector:
            readability = original_features.get('readability_score', 0.5)
            contrast = original_features.get('text_contrast', 0.5)
            has_cta = original_features.get('has_cta', 0)
            feature_vector['text_effectiveness'] = readability * contrast * (has_cta + 0.1)
        
        if 'simplicity_index' in feature_vector:
            comp_complex = original_features.get('composition_complexity', 0.5)
            overall_complex = original_features.get('overall_complexity', 0.5)
            feature_vector['simplicity_index'] = (2 - comp_complex - overall_complex) / 2
        
        # –î–æ–±–∞–≤–ª—è–µ–º –æ—Å—Ç–∞–ª—å–Ω—ã–µ –ø–æ –∞–Ω–∞–ª–æ–≥–∏–∏...
    
    def get_feature_importance(self, target: str = 'ctr') -> List[Tuple[str, float]]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å –∞–≥—Ä–µ–≥–∞—Ü–∏–µ–π –ø–æ –º–æ–¥–µ–ª—è–º."""
        if target not in self.feature_importance:
            return []
        
        # –ê–≥—Ä–µ–≥–∏—Ä—É–µ–º –≤–∞–∂–Ω–æ—Å—Ç—å –ø–æ –≤—Å–µ–º –º–æ–¥–µ–ª—è–º
        aggregated_importance = {}
        
        for model_name, importance_dict in self.feature_importance[target].items():
            weight = self.ensemble_weights[target].get(model_name, 0.3)
            
            for feature, importance in importance_dict.items():
                if feature not in aggregated_importance:
                    aggregated_importance[feature] = 0
                aggregated_importance[feature] += importance * weight
        
        # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –∏ —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞
        filtered_importance = [
            (feature, importance) for feature, importance in aggregated_importance.items()
            if importance >= FEATURE_IMPORTANCE_THRESHOLD
        ]
        
        filtered_importance.sort(key=lambda x: x[1], reverse=True)
        return filtered_importance[:20]  # –¢–æ–ø-20 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    
    def get_prediction_confidence(self, features: Dict[str, Any]) -> Dict[str, Tuple[float, float]]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –¥–æ–≤–µ—Ä–∏—Ç–µ–ª—å–Ω—ã—Ö –∏–Ω—Ç–µ—Ä–≤–∞–ª–æ–≤ —Å —É—á–µ—Ç–æ–º –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π."""
        if not self.is_trained:
            return {'ctr': (0.01, 0.05), 'conversion_rate': (0.02, 0.08), 'engagement': (0.05, 0.15)}
        
        # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –æ—Ç –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π
        predictions = self.predict(features)
        feature_vector = self._prepare_prediction_features(features)
        
        confidence_intervals = {}
        
        for target in ['ctr', 'conversion_rate', 'engagement']:
            feature_vector_scaled = self.scalers[target].transform([feature_vector])
            
            if target in self.feature_selectors:
                feature_vector_selected = self.feature_selectors[target].transform(feature_vector_scaled)
            else:
                feature_vector_selected = feature_vector_scaled
            
            # –°–æ–±–∏—Ä–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –æ—Ç –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π
            model_predictions = []
            for model_name, model in self.models[target].items():
                try:
                    if hasattr(model, 'predict'):
                        pred = model.predict(feature_vector_selected)[0]
                        model_predictions.append(pred)
                except:
                    continue
            
            if len(model_predictions) > 1:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ –º–µ–∂–¥—É –º–æ–¥–µ–ª—è–º–∏
                pred_mean = np.mean(model_predictions)
                pred_std = np.std(model_predictions)
                
                # 95% –¥–æ–≤–µ—Ä–∏—Ç–µ–ª—å–Ω—ã–π –∏–Ω—Ç–µ—Ä–≤–∞–ª
                margin = 1.96 * pred_std
                lower = max(pred_mean - margin, 0.001)
                upper = pred_mean + margin
                
                # –ü—Ä–∏–º–µ–Ω—è–µ–º –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è
                if target == 'ctr':
                    upper = min(upper, 0.15)
                elif target == 'conversion_rate':
                    upper = min(upper, 0.35)
                else:  # engagement
                    upper = min(upper, 0.8)
                
                confidence_intervals[target] = (lower, upper)
            else:
                # Fallback –∏–Ω—Ç–µ—Ä–≤–∞–ª—ã
                base_pred = predictions[target]
                margin = base_pred * 0.3  # 30% –æ—Ç –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
                confidence_intervals[target] = (
                    max(base_pred - margin, 0.001),
                    base_pred + margin
                )
        
        return confidence_intervals
    
    def explain_prediction(self, features: Dict[str, Any], target: str = 'ctr') -> Dict[str, Any]:
        """–î–µ—Ç–∞–ª—å–Ω–æ–µ –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è."""
        predictions = self.predict(features)
        confidence = self.get_prediction_confidence(features)
        feature_importance = self.get_feature_importance(target)
        
        # –ê–Ω–∞–ª–∏–∑ –∫–ª—é—á–µ–≤—ã—Ö —Ñ–∞–∫—Ç–æ—Ä–æ–≤ –¥–ª—è –¥–∞–Ω–Ω–æ–≥–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
        feature_impacts = self._analyze_feature_impacts(features, target, feature_importance)
        
        # –û–±—â–∞—è –æ—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞
        overall_score = self._calculate_overall_score(predictions)
        
        explanation = {
            'predictions': predictions,
            'confidence_intervals': confidence,
            'overall_score': overall_score,
            'feature_impacts': feature_impacts,
            'performance_category': self._categorize_performance(overall_score),
            'key_insights': self._generate_advanced_insights(features, predictions, feature_impacts),
            'model_confidence': self._assess_model_confidence(target),
            'recommendation_priority': self._assess_recommendation_priority(predictions)
        }
        
        return explanation
    
    def _analyze_feature_impacts(self, features: Dict, target: str, importance_list: List) -> List[Dict]:
        """–ê–Ω–∞–ª–∏–∑ –≤–ª–∏—è–Ω–∏—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π –ø—Ä–∏–∑–Ω–∞–∫–æ–≤."""
        impacts = []
        
        for feature_name, importance in importance_list[:10]:
            feature_value = features.get(feature_name, 0.5)
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –≤–ª–∏—è–Ω–∏–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∑–Ω–∞—á–µ–Ω–∏—è –∏ –≤–∞–∂–Ω–æ—Å—Ç–∏
            if importance > 0.1:  # –û—á–µ–Ω—å –≤–∞–∂–Ω—ã–π –ø—Ä–∏–∑–Ω–∞–∫
                if feature_value > 0.7:
                    impact = "–æ—á–µ–Ω—å –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ"
                elif feature_value > 0.5:
                    impact = "–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ"
                elif feature_value > 0.3:
                    impact = "–Ω–µ–π—Ç—Ä–∞–ª—å–Ω–æ–µ"
                else:
                    impact = "–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ"
            else:
                impact = "–º–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ"
            
            impacts.append({
                'feature': feature_name,
                'value': feature_value,
                'importance': importance,
                'impact': impact,
                'contribution': importance * feature_value
            })
        
        return impacts
    
    def _calculate_overall_score(self, predictions: Dict) -> float:
        """–†–∞—Å—á–µ—Ç –æ–±—â–µ–π –æ—Ü–µ–Ω–∫–∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏."""
        ctr_score = predictions['ctr'] / PERFORMANCE_METRICS['ctr']['target']
        conv_score = predictions['conversion_rate'] / PERFORMANCE_METRICS['conversion_rate']['target']
        eng_score = predictions['engagement'] / PERFORMANCE_METRICS['engagement']['target']
        
        # –í–∑–≤–µ—à–µ–Ω–Ω–∞—è –æ—Ü–µ–Ω–∫–∞
        overall = (ctr_score * 0.4 + conv_score * 0.4 + eng_score * 0.2)
        return min(overall, 2.0)  # –ú–∞–∫—Å–∏–º—É–º 200% –æ—Ç —Ü–µ–ª–µ–≤—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
    
    def _categorize_performance(self, score: float) -> str:
        """–ö–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏."""
        if score >= 1.2:
            return "–ü—Ä–µ–≤–æ—Å—Ö–æ–¥–Ω–∞—è"
        elif score >= 1.0:
            return "–û—Ç–ª–∏—á–Ω–∞—è"
        elif score >= 0.8:
            return "–•–æ—Ä–æ—à–∞—è"
        elif score >= 0.6:
            return "–°—Ä–µ–¥–Ω—è—è"
        else:
            return "–¢—Ä–µ–±—É–µ—Ç —É–ª—É—á—à–µ–Ω–∏—è"
    
    def _generate_advanced_insights(self, features: Dict, predictions: Dict, impacts: List) -> List[str]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã—Ö –∏–Ω—Å–∞–π—Ç–æ–≤."""
        insights = []
        
        # –ê–Ω–∞–ª–∏–∑ —Å–∏–ª—å–Ω—ã—Ö —Å—Ç–æ—Ä–æ–Ω
        strong_features = [i for i in impacts if i['impact'] in ['–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ', '–æ—á–µ–Ω—å –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ']]
        if len(strong_features) >= 3:
            insights.append(f"–ö—Ä–µ–∞—Ç–∏–≤ –∏–º–µ–µ—Ç {len(strong_features)} —Å–∏–ª—å–Ω—ã—Ö –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫")
        
        # –ê–Ω–∞–ª–∏–∑ —Å–ª–∞–±—ã—Ö –º–µ—Å—Ç
        weak_features = [i for i in impacts if i['impact'] == '–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ']
        if weak_features:
            insights.append(f"–û–±–Ω–∞—Ä—É–∂–µ–Ω–æ {len(weak_features)} –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –æ–±–ª–∞—Å—Ç–∏ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è")
        
        # –ü—Ä–µ–¥–º–µ—Ç–Ω—ã–µ –∏–Ω—Å–∞–π—Ç—ã
        if predictions['ctr'] > PERFORMANCE_METRICS['ctr']['target'] * 1.2:
            insights.append("–í—ã—Å–æ–∫–∏–π –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä—É–µ–º—ã–π CTR —É–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞ —Å–∏–ª—å–Ω—É—é –≤–∏–∑—É–∞–ª—å–Ω—É—é –ø—Ä–∏–≤–ª–µ–∫–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å")
        
        if predictions['conversion_rate'] > PERFORMANCE_METRICS['conversion_rate']['target'] * 1.2:
            insights.append("–û—Ç–ª–∏—á–Ω–∞—è –∫–æ–Ω–≤–µ—Ä—Å–∏—è –ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ —É–±–µ–∂–¥–µ–Ω–∏–µ –∫ –¥–µ–π—Å—Ç–≤–∏—é")
        
        if features.get('has_cta', 0) and predictions['conversion_rate'] < PERFORMANCE_METRICS['conversion_rate']['target']:
            insights.append("–ù–µ—Å–º–æ—Ç—Ä—è –Ω–∞ –Ω–∞–ª–∏—á–∏–µ CTA, –∫–æ–Ω–≤–µ—Ä—Å–∏—è –º–æ–∂–µ—Ç –±—ã—Ç—å —É–ª—É—á—à–µ–Ω–∞")
        
        return insights[:5]
    
    def _assess_model_confidence(self, target: str) -> float:
        """–û—Ü–µ–Ω–∫–∞ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏."""
        if target in self.model_performance:
            # –ë–µ—Ä–µ–º —Å—Ä–µ–¥–Ω–∏–π R¬≤ –ø–æ –≤—Å–µ–º –º–æ–¥–µ–ª—è–º
            r2_scores = [
                result.get('r2_score', 0) for result in self.model_performance[target].values()
                if 'r2_score' in result
            ]
            return np.mean(r2_scores) if r2_scores else 0.5
        return 0.5
    
    def _assess_recommendation_priority(self, predictions: Dict) -> str:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π."""
        avg_performance = np.mean(list(predictions.values()))
        target_avg = np.mean([
            PERFORMANCE_METRICS['ctr']['target'],
            PERFORMANCE_METRICS['conversion_rate']['target'],
            PERFORMANCE_METRICS['engagement']['target']
        ])
        
        if avg_performance < target_avg * 0.7:
            return "–í—ã—Å–æ–∫–∏–π"
        elif avg_performance < target_avg:
            return "–°—Ä–µ–¥–Ω–∏–π"
        else:
            return "–ù–∏–∑–∫–∏–π"


# –ê–ª–∏–∞—Å –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
MLEngine = AdvancedMLEngine
